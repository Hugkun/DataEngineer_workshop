### hugkunスキルアップ体験会(仮)データエンジニアコース
フォルダ構成

hugkun_skillup_DE/
├── csv/    # スクレイピングによって取得したリンクや記事内容を保存する 各自で作成してください
├── python  # 今回スクレイピングを行うスクリプト保存場所
├   ├── utils/  # 共通して使用するスクリプト保存場所
├   ├    ├── driver.py   # スクレイピングをする際に必要なWebDriverの設定をするスクリプト   
├   ├── 01_scraping_sample.py    # urlからサイト内にあるメニューの飛び先を取得するスクリプト
├   ├── 02_scraping_articles.py  # 01_scraping_sample.pyで取得したリンクの内容を取得するスクリプト
├   ├── 03_upload_to_s3.py       # 02_scraping_articles.pyで取得した内容をaws s3にアップロードするスクリプト
├── .gitignore  # gitにpushしたくないファイルを除外する 
├── .env_sample # 環境設定ファイル.envに記載する内容例            
├── Readme   


### 動かし方
以下のライブラリをインストールした環境を作成し、01_scraping_sample.py、02_scraping_articles.py の順に実行(03_upload_to_s3.pyはファイル作成等がある)

webdriver-manager==4.0.2
selenium==4.32.0
numpy==2.0.2
pandas==2.2.3
boto3==1.38.19

python/01_scraping_sample.py 
    変更せず実行するとhttps://hugkun.com/内のメニュー項目に紐付いたリンクを取得し、csv下にそのデータを保存する
python/02_scraping_articles.py 
    01_scraping_sample.pyで取得したリンクの内容を取得し、csv下にそのデータを保存する

python/03_upload_to_s3.py 
    1.先にawsのアクセスキーとシークレットアクセスキーを発行する必要がある。発行方法　https://qiita.com/raimu_hosoda/items/24b587fe44ced5262722
    2.発行後 .env_sampleを参考に.envを作成する
    3.aws(https://aws.amazon.com/jp/)からコンソールにサインインをクリックし、aws consoleにログインする
    4.aws consoleでs3と検索し、s3でbucketを作成する(名前は自由)
    5.03_upload_to_s3.py内のBucketを先ほど作成したbucket名にする
    6.実行する。問題なく動作していればs3上にアップロードされる




